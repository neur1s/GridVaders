{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vanilla RNN v2\n",
        "Base code taken from Sofiya's LSTM implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import trange\n",
        "import torch.nn.functional as F\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.colors as mcolors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### tSNE Plotting function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_tsne(hidden_states, grid_positions, n_back, step_hidden=40, step_pos=40):\n",
        "    \n",
        "    assert step_hidden != 0, \"Step zero has trivial representation.\"\n",
        "\n",
        "    # Convert (x, y) to grid ID if needed\n",
        "    if isinstance(grid_positions[0], (tuple, list, np.ndarray)) and len(grid_positions[0]) == 2:\n",
        "        # It's (x, y), convert to flat ID assuming 5x5 grid\n",
        "        pos_ids = np.array([x * 5 + y for x, y in grid_positions])\n",
        "    else:\n",
        "        # Already flat IDs\n",
        "        pos_ids = np.array(grid_positions)\n",
        "\n",
        "    # Run t-SNE\n",
        "    np.random.seed(0)\n",
        "    transformed = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=SEED).fit_transform(hidden_states)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(x=transformed[:, 0], y=transformed[:, 1], hue=pos_ids.astype(str))\n",
        "    \n",
        "    plt.title(f\"t-SNE of Hidden States at step {step_hidden}\\n colored by grid position at step {step_pos} (n={n_back})\")\n",
        "    plt.xlabel(\"t-SNE Dim 1\")\n",
        "    plt.ylabel(\"t-SNE Dim 2\")\n",
        "   \n",
        "   # Reorder legend and place it outside the plot\n",
        "    handles, labels = plt.gca().get_legend_handles_labels()\n",
        "    labels, handles = zip(*sorted(zip(labels, handles), key=lambda x: int(x[0])))  # sort numerically\n",
        "    plt.legend(handles, labels, bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Grid Pos ID\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RNN implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Training N=0 ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training N=0:  76%|███████▌  | 38/50 [03:38<01:15,  6.31s/it]"
          ]
        }
      ],
      "source": [
        "from n_back_spatial_task import (  \n",
        "    create_n_back_dataset,\n",
        "    NBackDataset as ItaloNBackDataset\n",
        ")\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)  \n",
        "\n",
        "#--Configuration--\n",
        "GRID_SIZE = np.array([5, 5], dtype=int)  \n",
        "HIDDEN_SIZE = 256\n",
        "INPUT_SIZE = 4 \n",
        "OUTPUT_SIZE = GRID_SIZE[0] * GRID_SIZE[1] \n",
        "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")  \n",
        "SEQ_LENGTH = 50  \n",
        "BATCH_SIZE = 100\n",
        "LEARNING_RATE = 0.0005\n",
        "EPOCHS = 50\n",
        "BOUNDARY = 'strict'  \n",
        "\n",
        "class NBackDataset(ItaloNBackDataset):  # Inherit from Italo's class\n",
        "    def __init__(self, num_samples, n_back):\n",
        "        # Using Italo's create_n_back_dataset function\n",
        "        dataset = create_n_back_dataset(\n",
        "            num_samples=num_samples,\n",
        "            n=n_back,\n",
        "            max_length=SEQ_LENGTH,\n",
        "            grid_size=GRID_SIZE,\n",
        "            boundary=BOUNDARY\n",
        "        )\n",
        "        super().__init__(dataset.x, dataset.y)\n",
        "\n",
        "class NBackRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.RNN = nn.RNN(\n",
        "            input_size=4,\n",
        "            hidden_size=HIDDEN_SIZE,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            dropout=0.2\n",
        "        )\n",
        "        self.fc = nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n",
        "    \n",
        "        self.h0 = nn.Parameter(torch.randn(1, 1, HIDDEN_SIZE) * 0.05)\n",
        "        #self.c0 = nn.Parameter(torch.randn(1, 1, HIDDEN_SIZE) * 0.05) #not useful for Vanilla RNNs (cell state)\n",
        "        \n",
        "    def forward(self, x, batch_size):\n",
        "        x = F.one_hot(x, num_classes=4).float().to(DEVICE)\n",
        "        h0 = self.h0.expand(1, batch_size, -1).contiguous()\n",
        "        #c0 = self.c0.expand(1, batch_size, -1).contiguous() #uncomment if using LSTM\n",
        "        out, _ = self.RNN(x, h0) # if LSTM, use touple (H0, C0) instead\n",
        "        return self.fc(out)\n",
        "\n",
        "class NBackTrainer:\n",
        "    def __init__(self, n_back):\n",
        "        self.n_back = n_back\n",
        "        self.model = NBackRNN().to(DEVICE)\n",
        "        self.optimizer = optim.AdamW(self.model.parameters(), lr=LEARNING_RATE)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        \n",
        "    def train(self, epochs=EPOCHS):\n",
        "        train_loader = DataLoader(\n",
        "            NBackDataset(10_000, self.n_back),\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=True\n",
        "        )\n",
        "        \n",
        "        self.model.train()\n",
        "        for epoch in trange(epochs, desc=f\"Training N={self.n_back}\"):\n",
        "            total_loss = 0\n",
        "            for actions, targets in train_loader:\n",
        "                actions, targets = actions.to(DEVICE), targets.to(DEVICE)\n",
        "                batch_size = actions.size(0)\n",
        "                \n",
        "                self.optimizer.zero_grad()\n",
        "                logits = self.model(actions, batch_size)\n",
        "                \n",
        "                loss = self.criterion(\n",
        "                    logits[:, self.n_back:].reshape(-1, OUTPUT_SIZE),  # Skip first N logits\n",
        "                    targets[:, 1:].reshape(-1)  # Skip initial position in targets (align x and y)\n",
        "                    )\n",
        "                \n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "                self.optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "    \n",
        "    def evaluate(self, test_samples=1_000, return_hidden_states=False, time_step=None):\n",
        "        test_loader = DataLoader(\n",
        "            NBackDataset(test_samples, self.n_back),\n",
        "            batch_size=BATCH_SIZE\n",
        "        )\n",
        "        \n",
        "        self.model.eval()\n",
        "        correct = total = 0\n",
        "        all_hidden_states = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for actions, targets in test_loader:\n",
        "                actions, targets = actions.to(DEVICE), targets.to(DEVICE)\n",
        "                batch_size = actions.size(0)\n",
        "\n",
        "                # Get one-hot and hidden outputs\n",
        "                one_hot = F.one_hot(actions, num_classes=4).float()\n",
        "                h0 = self.model.h0.expand(1, batch_size, -1).contiguous()\n",
        "                c0 = self.model.c0.expand(1, batch_size, -1).contiguous()\n",
        "                RNN_out, _ = self.model.RNN(one_hot, (h0, c0))\n",
        "\n",
        "                logits = self.model.fc(RNN_out)\n",
        "                preds = logits[:, self.n_back:].argmax(-1)\n",
        "                tgts = targets[:, 1:]  # Remove the initial dummy target\n",
        "                correct += (preds == tgts).sum().item()\n",
        "                total += tgts.numel()\n",
        "\n",
        "                if return_hidden_states and time_step is not None and time_step < SEQ_LENGTH:\n",
        "                    valid_batch_idx = (targets.shape[1] > time_step + 1)\n",
        "                    for i in range(batch_size):\n",
        "                        if valid_batch_idx:\n",
        "                            hidden = RNN_out[i, time_step, :].cpu().numpy()\n",
        "                            label = targets[i, time_step + 1].item()\n",
        "                            all_hidden_states.append(hidden)\n",
        "                            all_labels.append(label)\n",
        "\n",
        "        acc = correct / total if total > 0 else 0\n",
        "        if return_hidden_states:\n",
        "            return acc, np.array(all_hidden_states), np.array(all_labels)\n",
        "        return acc\n",
        "\n",
        "def run_experiment(max_n=10):\n",
        "    accuracies = []\n",
        "    for N in range(max_n + 1):\n",
        "        print(f\"\\n=== Training N={N} ===\")\n",
        "        trainer = NBackTrainer(N)\n",
        "        trainer.train()\n",
        "\n",
        "        # Evaluate 3 times for stability\n",
        "        if N in [0]:  # For selected N, extract hidden states too\n",
        "            print(f\"Evaluating and extracting hidden states for N={N}...\")\n",
        "            acc, hidden_states, labels = trainer.evaluate(return_hidden_states=True, time_step=40)\n",
        "            accuracies.append(acc)\n",
        "            print(f\"N={N} Final Accuracy: {acc:.4f}\")\n",
        "            print(\"Running t-SNE...\")\n",
        "            plot_tsne(hidden_states, labels, N)\n",
        "        else:\n",
        "            eval_acc = np.mean([trainer.evaluate() for _ in range(3)])\n",
        "            accuracies.append(eval_acc)\n",
        "            print(f\"N={N} Final Accuracy: {eval_acc:.4f}\")\n",
        "            \n",
        "    return accuracies\n",
        "\n",
        "# Run experiment\n",
        "accuracies = run_experiment()\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(len(accuracies)), accuracies, marker='o', linewidth=2, markersize=8)\n",
        "plt.xlabel(\"N-back\", fontsize=12)\n",
        "plt.ylabel(\"Accuracy\", fontsize=12)\n",
        "plt.xticks(range(len(accuracies)))\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.ylim(0, 1.05)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "neuroai-2025",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
